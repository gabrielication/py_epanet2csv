{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-07-28T11:34:21.366446Z",
     "end_time": "2023-07-28T11:34:24.702714Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (12, 10)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "METRICS = [\n",
    "\tkeras.metrics.TruePositives(name='tp'),\n",
    "\tkeras.metrics.FalsePositives(name='fp'),\n",
    "\tkeras.metrics.TrueNegatives(name='tn'),\n",
    "\tkeras.metrics.FalseNegatives(name='fn'),\n",
    "\tkeras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "\tkeras.metrics.Precision(name='precision'),\n",
    "\tkeras.metrics.Recall(name='recall'),\n",
    "\tkeras.metrics.AUC(name='auc'),\n",
    "\tkeras.metrics.AUC(name='prc', curve='PR'),  # precision-recall curve\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-28T11:34:24.705716Z",
     "end_time": "2023-07-28T11:34:24.771750Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def plot_loss(history, label, n):\n",
    "\t# Use a log scale on y-axis to show the wide range of values.\n",
    "\tplt.semilogy(history.epoch, history.history['loss'],\n",
    "\t\t\t   color=colors[n], label='Train ' + label)\n",
    "\tplt.semilogy(history.epoch, history.history['val_loss'],\n",
    "\t\t\t   color=colors[n], label='Val ' + label,\n",
    "\t\t\t   linestyle=\"--\")\n",
    "\tplt.xlabel('Epoch')\n",
    "\tplt.ylabel('Loss')\n",
    "\tplt.legend()\n",
    "\n",
    "\n",
    "def plot_metrics(history):\n",
    "\tmetrics = ['loss', 'prc', 'precision', 'recall']\n",
    "\tfor n, metric in enumerate(metrics):\n",
    "\t\tname = metric.replace(\"_\", \" \").capitalize()\n",
    "\t\tplt.subplot(2, 2, n + 1)\n",
    "\t\tplt.plot(history.epoch, history.history[metric], color=colors[0], label='Train')\n",
    "\t\tplt.plot(history.epoch, history.history['val_' + metric],\n",
    "\t\t\t\t color=colors[0], linestyle=\"--\", label='Val')\n",
    "\t\tplt.xlabel('Epoch')\n",
    "\t\tplt.ylabel(name)\n",
    "\t\tif metric == 'loss':\n",
    "\t\t\tplt.ylim([0, plt.ylim()[1]])\n",
    "\t\telif metric == 'auc':\n",
    "\t\t\tplt.ylim([0.8, 1.1])\n",
    "\t\telse:\n",
    "\t\t\tplt.ylim([0, 1.1])\n",
    "\n",
    "\t\tplt.legend();\n",
    "\n",
    "def plot_cm(labels, predictions, title, p=0.5):\n",
    "\tcm = confusion_matrix(labels, predictions > p)\n",
    "\tplt.figure(figsize=(5, 5))\n",
    "\tsns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "\t# plt.title('Confusion matrix @{:.2f}'.format(p))\n",
    "\tplt.title(title)\n",
    "\tplt.ylabel('Actual label')\n",
    "\tplt.xlabel('Predicted label')\n",
    "\n",
    "\tprint('Legitimate Transactions Detected (True Negatives): ', cm[0][0])\n",
    "\tprint('Legitimate Transactions Incorrectly Detected (False Positives): ', cm[0][1])\n",
    "\tprint('Fraudulent Transactions Missed (False Negatives): ', cm[1][0])\n",
    "\tprint('Fraudulent Transactions Detected (True Positives): ', cm[1][1])\n",
    "\tprint('Total Fraudulent Transactions: ', np.sum(cm[1]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-28T11:34:24.780713Z",
     "end_time": "2023-07-28T11:34:24.786715Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "dataset_path = 'tensorflow_group_datasets/one_res_small/0_no_leaks_rand_base_demand/'\n",
    "\n",
    "out_filename = '1M_one_res_small_leaks_ordered_group_0_node_0_0164_merged.csv'\n",
    "raw_df1 = pd.read_csv(dataset_path+out_filename, delimiter=\";\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-28T11:34:24.789728Z",
     "end_time": "2023-07-28T11:34:25.254471Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 2048\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-28T11:34:25.253474Z",
     "end_time": "2023-07-28T11:34:25.265484Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "\n",
    "leak_area = \"0164\" # \"0246\" #\"0164\"\n",
    "# 1-->7 2-->7 3-->6 4-->2 5-->1 6-->7 7-->7\n",
    "leak_group = 3\n",
    "leak_group_model = 7\n",
    "\n",
    "dataset_path = \"tensorflow_group_datasets/model/h5/\"\n",
    "model_filename = \"model_leak_group\"+str(leak_group)+\"_train_node_\"+str(leak_group_model)+\".h5\"\n",
    "loaded_model = tf.keras.models.load_model(dataset_path+model_filename)\n",
    "\n",
    "# tf.keras.utils.plot_model(loaded_model, to_file='model_plot.png', show_shapes=True)\n",
    "\n",
    "# tf.keras.utils.plot_model(\n",
    "#     loaded_model,\n",
    "#     to_file='tensorflow_group_datasets/model_plot.png',\n",
    "#     show_shapes=True,\n",
    "#     show_dtype=False,\n",
    "#     show_layer_names=True,\n",
    "#     rankdir='TB',\n",
    "#     expand_nested=False,\n",
    "#     dpi=96,\n",
    "#     layer_range=None,\n",
    "#     show_layer_activations=False,\n",
    "# )\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-28T11:34:25.268481Z",
     "end_time": "2023-07-28T11:34:25.466554Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leak node :  1\n",
      "Examples:\n",
      "    Total: 107520\n",
      "    Positive: 6720 (6.25% of total)\n",
      "\n",
      "*********** SCALER ************\n",
      "[4.98433817e-03 2.63751432e-03 3.12095934e+01 1.32352537e+01\n",
      " 4.98022536e+05 1.37824692e+06 1.14933558e-01 2.87392810e-03\n",
      " 3.14851694e+01 1.34203503e+01 2.77795988e-03 3.23093247e+01\n",
      " 1.40876168e+01 2.79106923e-03 3.26265741e+01 1.43070326e+01\n",
      " 2.59472214e-03 3.18982540e+01 1.45594731e+01 2.68297108e-03\n",
      " 3.22212472e+01 1.37564505e+01 2.62765254e-03 3.16299471e+01\n",
      " 1.35611076e+01 2.54395277e-03 2.96899570e+01 1.18688538e+01\n",
      " 2.62326116e-03 3.17111525e+01 1.36133978e+01 2.50040637e-03\n",
      " 2.94561603e+01 1.17711583e+01 2.37360481e-03 2.89513752e+01\n",
      " 1.12559479e+01 8.85440296e-02]\n",
      "[8.30404004e-06 5.26634047e-06 1.51048048e+02 1.50752481e+02\n",
      " 4.77189983e+06 8.73212195e+05 4.41101979e-03 5.30731319e-06\n",
      " 1.21726950e+02 1.17115282e+02 5.68217142e-06 1.63204466e+02\n",
      " 1.49951715e+02 5.31387135e-06 1.67472608e+02 1.69488766e+02\n",
      " 5.51925442e-06 1.97377489e+02 2.05534423e+02 5.32758657e-06\n",
      " 1.64462782e+02 1.60220885e+02 5.23896657e-06 1.58542014e+02\n",
      " 1.54714104e+02 5.33495609e-06 1.22152752e+02 1.16077033e+02\n",
      " 5.34724315e-06 1.56837905e+02 1.66357389e+02 5.02104570e-06\n",
      " 1.22302428e+02 1.27289005e+02 4.44456421e-06 1.16008438e+02\n",
      " 1.23580042e+02 2.84393035e-03]\n",
      "leak node :  2\n",
      "Examples:\n",
      "    Total: 107520\n",
      "    Positive: 6720 (6.25% of total)\n",
      "\n",
      "*********** SCALER ************\n",
      "[4.99779849e-03 2.68016630e-03 3.12050951e+01 1.32313904e+01\n",
      " 4.98016740e+05 1.37824795e+06 1.16878229e-01 2.85711643e-03\n",
      " 3.14269223e+01 1.33639393e+01 2.87829637e-03 3.23626646e+01\n",
      " 1.41416218e+01 2.62022304e-03 3.23188273e+01 1.39997599e+01\n",
      " 2.78090090e-03 3.21268131e+01 1.47885742e+01 2.68351614e-03\n",
      " 3.21103942e+01 1.36431524e+01 2.67993433e-03 3.16748339e+01\n",
      " 1.36081149e+01 2.65772298e-03 2.98690921e+01 1.20462596e+01\n",
      " 2.66551142e-03 3.17188793e+01 1.36215232e+01 2.55887785e-03\n",
      " 2.94599647e+01 1.17762160e+01 2.44125624e-03 2.89583323e+01\n",
      " 1.12652488e+01 9.00548729e-02]\n",
      "[8.32717155e-06 5.03234233e-06 1.50850707e+02 1.50761753e+02\n",
      " 4.78959559e+06 8.74753039e+05 4.38225381e-03 5.31218478e-06\n",
      " 1.22800298e+02 1.18121711e+02 5.21910218e-06 1.62546608e+02\n",
      " 1.49277554e+02 5.60415140e-06 1.73913832e+02 1.76167434e+02\n",
      " 5.07504986e-06 1.93835417e+02 2.02379178e+02 5.07439950e-06\n",
      " 1.67036402e+02 1.62874705e+02 4.95018869e-06 1.58211478e+02\n",
      " 1.54468530e+02 4.92736699e-06 1.19118661e+02 1.13362276e+02\n",
      " 4.93610979e-06 1.56722422e+02 1.66255056e+02 4.91082592e-06\n",
      " 1.22691657e+02 1.27861616e+02 4.18282334e-06 1.16379034e+02\n",
      " 1.24098254e+02 2.88826950e-03]\n",
      "leak node :  3\n",
      "Examples:\n",
      "    Total: 107520\n",
      "    Positive: 6720 (6.25% of total)\n",
      "\n",
      "*********** SCALER ************\n",
      "[5.00330567e-03 2.64265110e-03 3.11900067e+01 1.32141038e+01\n",
      " 4.98026886e+05 1.37825048e+06 1.14636585e-01 2.88072209e-03\n",
      " 3.14792960e+01 1.34151524e+01 2.80827309e-03 3.23057961e+01\n",
      " 1.40855550e+01 2.78122061e-03 3.25950980e+01 1.42752712e+01\n",
      " 2.59525698e-03 3.18909166e+01 1.45514435e+01 2.67927827e-03\n",
      " 3.22041176e+01 1.37380723e+01 2.64710016e-03 3.16097168e+01\n",
      " 1.35415395e+01 2.53670343e-03 2.96682932e+01 1.18467507e+01\n",
      " 2.62233562e-03 3.17073915e+01 1.36071563e+01 2.49377928e-03\n",
      " 2.94502314e+01 1.17671727e+01 2.39668448e-03 2.89500011e+01\n",
      " 1.12569415e+01 8.81952312e-02]\n",
      "[8.34384992e-06 5.25255417e-06 1.50571516e+02 1.50213507e+02\n",
      " 4.76615346e+06 8.75273213e+05 4.44725848e-03 5.34846204e-06\n",
      " 1.21962620e+02 1.17320157e+02 5.44713776e-06 1.62564067e+02\n",
      " 1.49324728e+02 5.32531751e-06 1.67263166e+02 1.69291171e+02\n",
      " 5.58491080e-06 1.96819185e+02 2.05027092e+02 5.30264994e-06\n",
      " 1.64271112e+02 1.59973447e+02 5.33989379e-06 1.58454421e+02\n",
      " 1.54608377e+02 5.26847030e-06 1.21072719e+02 1.15060083e+02\n",
      " 5.22117808e-06 1.55728098e+02 1.65117242e+02 5.01119853e-06\n",
      " 1.23329590e+02 1.28395432e+02 4.70237755e-06 1.17167587e+02\n",
      " 1.24826147e+02 2.87019702e-03]\n",
      "leak node :  4\n",
      "Examples:\n",
      "    Total: 107520\n",
      "    Positive: 6720 (6.25% of total)\n",
      "\n",
      "*********** SCALER ************\n",
      "[4.98908380e-03 2.70014984e-03 3.13391211e+01 1.33624245e+01\n",
      " 4.98012683e+05 1.37824703e+06 1.14520305e-01 2.85578380e-03\n",
      " 3.14910142e+01 1.34272027e+01 2.88713165e-03 3.24761749e+01\n",
      " 1.42540044e+01 2.84487885e-03 3.27680732e+01 1.44488027e+01\n",
      " 2.75775547e-03 3.21817689e+01 1.48449416e+01 2.66649091e-03\n",
      " 3.22329809e+01 1.37658645e+01 2.64971743e-03 3.17334377e+01\n",
      " 1.36657472e+01 2.58517235e-03 2.97655069e+01 1.19443606e+01\n",
      " 2.63083683e-03 3.18108119e+01 1.37138102e+01 2.62981386e-03\n",
      " 2.96734801e+01 1.19904884e+01 2.51651246e-03 2.91671727e+01\n",
      " 1.14746040e+01 8.74962113e-02]\n",
      "[8.35482815e-06 5.09464050e-06 1.50506075e+02 1.50284815e+02\n",
      " 4.79236805e+06 8.71419144e+05 4.48901170e-03 5.56411397e-06\n",
      " 1.23818914e+02 1.19299669e+02 5.33595410e-06 1.61955790e+02\n",
      " 1.48760966e+02 5.02572604e-06 1.66810841e+02 1.68863485e+02\n",
      " 5.00873671e-06 1.93987820e+02 2.02289276e+02 5.30630798e-06\n",
      " 1.66745795e+02 1.62460358e+02 5.15276156e-06 1.58707991e+02\n",
      " 1.54967899e+02 5.28583944e-06 1.22231732e+02 1.16468082e+02\n",
      " 5.17287877e-06 1.56785932e+02 1.66302088e+02 4.82306751e-06\n",
      " 1.21093898e+02 1.26202422e+02 4.23274161e-06 1.14928973e+02\n",
      " 1.22621225e+02 2.94254133e-03]\n",
      "leak node :  5\n",
      "Examples:\n",
      "    Total: 107520\n",
      "    Positive: 6720 (6.25% of total)\n",
      "\n",
      "*********** SCALER ************\n",
      "[5.00716312e-03 2.66945109e-03 3.12331708e+01 1.32540773e+01\n",
      " 4.98018886e+05 1.37824868e+06 1.15031568e-01 2.85392979e-03\n",
      " 3.14519455e+01 1.33858294e+01 2.82771628e-03 3.23736121e+01\n",
      " 1.41497023e+01 2.84275342e-03 3.26635350e+01 1.43462388e+01\n",
      " 2.72306799e-03 3.20702638e+01 1.47293122e+01 2.64872680e-03\n",
      " 3.21814744e+01 1.37166171e+01 2.62862913e-03 3.16156393e+01\n",
      " 1.35470335e+01 2.52204180e-03 2.97079232e+01 1.18852311e+01\n",
      " 2.58045862e-03 3.16969707e+01 1.35991938e+01 2.59179403e-03\n",
      " 2.95676939e+01 1.18808559e+01 2.48262567e-03 2.90615015e+01\n",
      " 1.13647370e+01 8.83298244e-02]\n",
      "[8.32230142e-06 5.16372145e-06 1.50079366e+02 1.49893331e+02\n",
      " 4.76414208e+06 8.75799951e+05 4.45907864e-03 5.55385447e-06\n",
      " 1.22904517e+02 1.18398220e+02 5.44836747e-06 1.62463485e+02\n",
      " 1.49320066e+02 5.04581321e-06 1.67209145e+02 1.69219050e+02\n",
      " 5.07490823e-06 1.94622401e+02 2.02948148e+02 5.26022103e-06\n",
      " 1.65949217e+02 1.61662053e+02 5.36143191e-06 1.59337000e+02\n",
      " 1.55617923e+02 5.23985399e-06 1.21748092e+02 1.15962504e+02\n",
      " 5.35970535e-06 1.57412066e+02 1.66929078e+02 4.84544643e-06\n",
      " 1.21384962e+02 1.26513751e+02 4.31864611e-06 1.15210691e+02\n",
      " 1.22916265e+02 2.90537470e-03]\n",
      "leak node :  6\n",
      "Examples:\n",
      "    Total: 107520\n",
      "    Positive: 6720 (6.25% of total)\n",
      "\n",
      "*********** SCALER ************\n",
      "[4.99518087e-03 2.69925229e-03 3.12660122e+01 1.32857281e+01\n",
      " 4.98020578e+05 1.37824985e+06 1.14357112e-01 2.82512507e-03\n",
      " 3.14427018e+01 1.33790201e+01 2.86515609e-03 3.24195473e+01\n",
      " 1.41987592e+01 2.84245610e-03 3.27065445e+01 1.43875754e+01\n",
      " 2.75254577e-03 3.21190627e+01 1.47771157e+01 2.69074013e-03\n",
      " 3.21689286e+01 1.37033873e+01 2.66928522e-03 3.16693730e+01\n",
      " 1.36020425e+01 2.57929709e-03 2.97162161e+01 1.18945836e+01\n",
      " 2.62901039e-03 3.17496077e+01 1.36518308e+01 2.60355431e-03\n",
      " 2.96152760e+01 1.19283714e+01 2.51668059e-03 2.91093697e+01\n",
      " 1.14128794e+01 8.73832607e-02]\n",
      "[8.34592862e-06 5.14177592e-06 1.49307059e+02 1.49076799e+02\n",
      " 4.75865002e+06 8.73332189e+05 4.47458650e-03 5.50636942e-06\n",
      " 1.23136169e+02 1.18642055e+02 5.37836562e-06 1.61234977e+02\n",
      " 1.48107223e+02 5.03408237e-06 1.66145055e+02 1.68173396e+02\n",
      " 4.97247530e-06 1.93153503e+02 2.01463937e+02 5.38621791e-06\n",
      " 1.65901819e+02 1.61647681e+02 5.25561086e-06 1.58005338e+02\n",
      " 1.54278654e+02 5.26227925e-06 1.21872523e+02 1.16147265e+02\n",
      " 5.14196759e-06 1.56312793e+02 1.65815567e+02 4.76814175e-06\n",
      " 1.20251788e+02 1.25330031e+02 4.26176669e-06 1.14070345e+02\n",
      " 1.21704033e+02 2.92607649e-03]\n",
      "leak node :  7\n",
      "Examples:\n",
      "    Total: 107520\n",
      "    Positive: 6720 (6.25% of total)\n",
      "\n",
      "*********** SCALER ************\n",
      "[5.00198226e-03 2.64952372e-03 3.12243013e+01 1.32468656e+01\n",
      " 4.98022444e+05 1.37825022e+06 1.15400191e-01 2.83045698e-03\n",
      " 3.14663551e+01 1.34034848e+01 2.79682583e-03 3.23319420e+01\n",
      " 1.41117761e+01 2.81829260e-03 3.26406508e+01 1.43217710e+01\n",
      " 2.70748389e-03 3.20236272e+01 1.46855092e+01 2.65871563e-03\n",
      " 3.21940850e+01 1.37265573e+01 2.61688574e-03 3.16207255e+01\n",
      " 1.35543587e+01 2.54760310e-03 2.96924266e+01 1.18692428e+01\n",
      " 2.53277893e-03 3.16535908e+01 1.35543965e+01 2.55071399e-03\n",
      " 2.95489061e+01 1.18665758e+01 2.44462956e-03 2.90450200e+01\n",
      " 1.13532948e+01 8.88958045e-02]\n",
      "[8.31187782e-06 5.21348177e-06 1.50949833e+02 1.50651280e+02\n",
      " 4.78364859e+06 8.75110515e+05 4.43328324e-03 5.49164614e-06\n",
      " 1.23060885e+02 1.18466403e+02 5.52334230e-06 1.63204580e+02\n",
      " 1.49975370e+02 5.24024736e-06 1.67615562e+02 1.69659492e+02\n",
      " 5.23815471e-06 1.95554316e+02 2.03844099e+02 5.35502433e-06\n",
      " 1.65706846e+02 1.61411034e+02 5.29582504e-06 1.59349703e+02\n",
      " 1.55565760e+02 5.44476496e-06 1.21326329e+02 1.15487415e+02\n",
      " 5.35026969e-06 1.57904056e+02 1.67368069e+02 4.81684101e-06\n",
      " 1.22523368e+02 1.27675172e+02 4.37053738e-06 1.16372407e+02\n",
      " 1.24114506e+02 2.87437708e-03]\n",
      "leak node :  8\n",
      "Examples:\n",
      "    Total: 107520\n",
      "    Positive: 6720 (6.25% of total)\n",
      "\n",
      "*********** SCALER ************\n",
      "[5.00167572e-03 2.66185744e-03 3.12548373e+01 1.32776063e+01\n",
      " 4.98020623e+05 1.37824909e+06 1.14589832e-01 2.84301577e-03\n",
      " 3.14728420e+01 1.34099966e+01 2.85852836e-03 3.24028768e+01\n",
      " 1.41820559e+01 2.76427830e-03 3.26317947e+01 1.43135662e+01\n",
      " 2.71067541e-03 3.20995719e+01 1.47639040e+01 2.70638164e-03\n",
      " 3.22988935e+01 1.38317279e+01 2.66175636e-03 3.17163010e+01\n",
      " 1.36502863e+01 2.59606165e-03 2.98320414e+01 1.20114404e+01\n",
      " 2.67630230e-03 3.18697046e+01 1.37719942e+01 2.40933947e-03\n",
      " 2.93473977e+01 1.16653308e+01 2.38316331e-03 2.89744032e+01\n",
      " 1.12825929e+01 8.79803296e-02]\n",
      "[8.29775958e-06 5.19511089e-06 1.50520739e+02 1.50190331e+02\n",
      " 4.77556563e+06 8.71616901e+05 4.44049441e-03 5.49568183e-06\n",
      " 1.23674970e+02 1.18825868e+02 5.32133581e-06 1.62808601e+02\n",
      " 1.49372620e+02 5.29104212e-06 1.68731549e+02 1.70764882e+02\n",
      " 5.21677128e-06 1.94804008e+02 2.03110626e+02 5.16878013e-06\n",
      " 1.64516103e+02 1.60268862e+02 5.22657274e-06 1.58560716e+02\n",
      " 1.54648286e+02 5.18169507e-06 1.20847557e+02 1.14881847e+02\n",
      " 4.94471312e-06 1.55052323e+02 1.64513663e+02 5.39502913e-06\n",
      " 1.25776238e+02 1.30675255e+02 4.54792307e-06 1.17300867e+02\n",
      " 1.24880772e+02 2.88143305e-03]\n",
      "leak node :  9\n",
      "Examples:\n",
      "    Total: 107520\n",
      "    Positive: 6720 (6.25% of total)\n",
      "\n",
      "*********** SCALER ************\n",
      "[5.00694820e-03 2.65787055e-03 3.12544309e+01 1.32795060e+01\n",
      " 4.98017808e+05 1.37825163e+06 1.14744532e-01 2.88494878e-03\n",
      " 3.15494265e+01 1.34862345e+01 2.85135833e-03 3.24202263e+01\n",
      " 1.41992371e+01 2.78571679e-03 3.26546161e+01 1.43353159e+01\n",
      " 2.67660127e-03 3.20176929e+01 1.46791321e+01 2.70361413e-03\n",
      " 3.22803609e+01 1.38134883e+01 2.62914951e-03 3.16655390e+01\n",
      " 1.35984451e+01 2.53609330e-03 2.97539044e+01 1.19305054e+01\n",
      " 2.66742228e-03 3.18382726e+01 1.37408279e+01 2.47970642e-03\n",
      " 2.94329032e+01 1.17488553e+01 2.33847755e-03 2.89226673e+01\n",
      " 1.12291250e+01 8.81914437e-02]\n",
      "[8.31821318e-06 5.21324894e-06 1.51195751e+02 1.50912081e+02\n",
      " 4.77808770e+06 8.76357033e+05 4.42903854e-03 5.28890664e-06\n",
      " 1.22400575e+02 1.17669148e+02 5.24547728e-06 1.62506189e+02\n",
      " 1.49164100e+02 5.31362301e-06 1.68435180e+02 1.70453812e+02\n",
      " 5.49896521e-06 1.96341416e+02 2.04638597e+02 5.16165227e-06\n",
      " 1.64954417e+02 1.60685736e+02 5.26929888e-06 1.59610847e+02\n",
      " 1.55657308e+02 5.14144685e-06 1.22188333e+02 1.16171438e+02\n",
      " 5.10846379e-06 1.55575948e+02 1.65052137e+02 5.17533394e-06\n",
      " 1.24273246e+02 1.29273037e+02 4.71437687e-06 1.18018010e+02\n",
      " 1.25617756e+02 2.86995321e-03]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1200x1000 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "leak_area = \"0164\" # \"0246\" #\"0164\"\n",
    "# 1-->7 2-->7 3-->6 4-->2 5-->1 6-->7 7-->7\n",
    "leak_group = 7\n",
    "leak_group_model = 7\n",
    "\n",
    "dataset_path = \"tensorflow_group_datasets/model/h5/\"\n",
    "model_filename = \"model_leak_group\"+str(leak_group)+\"_train_node_\"+str(leak_group_model)+\".h5\"\n",
    "loaded_model = tf.keras.models.load_model(dataset_path+model_filename)\n",
    "\n",
    "dataset_path = 'tensorflow_group_datasets/one_res_small/1_at_82_leaks_rand_base_demand/'\n",
    "\n",
    "# plt.figure(figsize=(5, 5))\n",
    "fig = plt.figure()\n",
    "\n",
    "\n",
    "for leak_node in range(1,10,1):\n",
    "\tprint(\"leak node : \", leak_node)\n",
    "\tout_filename = \"1M_one_res_small_leaks_ordered_group_\"+str(leak_group)+\"_node_\"+str(leak_node)+\"_\"+leak_area+\"_merged.csv\"\n",
    "\traw_df2 = pd.read_csv(dataset_path+out_filename, delimiter=\";\")\n",
    "\n",
    "    # Appending multiple DataFrame\n",
    "\traw_df = pd.concat([raw_df1, raw_df2])\n",
    "\traw_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\tcleaned_df = raw_df.copy()\n",
    "\tpop_col = ['hour', 'nodeID', 'node_type', 'leak_area_value', 'leak_discharge_value', 'leak_demand_value', 'has_leak' ]\n",
    "\n",
    "\tcleaned_df = cleaned_df.drop(pop_col, axis=1)\n",
    "\tcleaned_df.rename(columns = {'leak_group':'Class'}, inplace = True)\n",
    "\n",
    "\t#%%\n",
    "\tneg, pos = np.bincount(cleaned_df['Class'])\n",
    "\ttotal = neg + pos\n",
    "\tprint('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n",
    "\t\ttotal, pos, 100 * pos / total))\n",
    "\n",
    "\t#%%\n",
    "\t# Use a utility from sklearn to split and shuffle your dataset.\n",
    "\ttrain_df, test_df = train_test_split(cleaned_df, test_size=0.2)\n",
    "\ttrain_df, val_df = train_test_split(train_df, test_size=0.2)\n",
    "\n",
    "\t# Form np arrays of labels and features.\n",
    "\ttrain_labels = np.array(train_df.pop('Class'))\n",
    "\tbool_train_labels = train_labels != 0\n",
    "\tval_labels = np.array(val_df.pop('Class'))\n",
    "\ttest_labels = np.array(test_df.pop('Class'))\n",
    "\n",
    "\ttrain_features = np.array(train_df)\n",
    "\tval_features = np.array(val_df)\n",
    "\ttest_features = np.array(test_df)\n",
    "\n",
    "\t#%%\n",
    "\tscaler = StandardScaler()\n",
    "\ttrain_features = scaler.fit_transform(train_features)\n",
    "\tval_features = scaler.transform(val_features)\n",
    "\ttest_features = scaler.transform(test_features)\n",
    "\tprint(\"*********** SCALER ************\")\n",
    "\tprint(scaler.mean_)\n",
    "\tprint(scaler.var_)\n",
    "\tcontinue\n",
    "\n",
    "\n",
    "\n",
    "\t#%%\n",
    "\t# pos_df = pd.DataFrame(train_features[ bool_train_labels], columns=train_df.columns)\n",
    "\t# neg_df = pd.DataFrame(train_features[~bool_train_labels], columns=train_df.columns)\n",
    "\n",
    "\n",
    "\t# baseline_results =loaded_model.evaluate(train_features, train_labels, batch_size=BATCH_SIZE, verbose=0)\n",
    "\t# for name, value in zip(loaded_model.metrics_names, baseline_results):\n",
    "\t# \tif name == \"loss\":\n",
    "\t# \t\tloss = value\n",
    "\t# \t\tprint(name, ': ', value)\n",
    "\t# \tif name == \"accuracy\":\n",
    "\t# \t\tacc = value\n",
    "\t# \tprint(name, ': ', value)\n",
    "\t# print()\n",
    "\t#\n",
    "\t#\n",
    "\t# baseline_results =loaded_model.evaluate(val_features, val_labels, batch_size=BATCH_SIZE, verbose=0)\n",
    "\t# for name, value in zip(loaded_model.metrics_names, baseline_results):\n",
    "\t# \tif name == \"loss\":\n",
    "\t# \t\tloss = value\n",
    "\t# \t\tprint(name, ': ', value)\n",
    "\t# \tif name == \"accuracy\":\n",
    "\t# \t\tacc = value\n",
    "\t# \tprint(name, ': ', value)\n",
    "\t# print()\n",
    "\n",
    "\tbaseline_results =loaded_model.evaluate(test_features, test_labels, batch_size=BATCH_SIZE, verbose=0)\n",
    "\tfor name, value in zip(loaded_model.metrics_names, baseline_results):\n",
    "\t\t# print(name, ': ', value)\n",
    "\t\tif name == \"loss\":\n",
    "\t\t\tloss = value\n",
    "\t\t\tprint(name, ': ', value)\n",
    "\t\tif name == \"accuracy\":\n",
    "\t\t\tacc = value\n",
    "\t\t\tprint(name, ': ', value)\n",
    "\n",
    "\ttest_predictions_baseline = loaded_model.predict(test_features, batch_size=BATCH_SIZE)\n",
    "\n",
    "\t# plot_cm(test_labels, test_predictions_baseline, \"leak node : \"+str(leak_node))\n",
    "\tp=0.5\n",
    "\n",
    "\tcm = confusion_matrix(test_labels, test_predictions_baseline > p)\n",
    "\t# plt.figure(figsize=(5, 5))\n",
    "\tax1 = fig.add_subplot(3, 3, leak_node)\n",
    "\tsns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "\n",
    "\tplt.title(\"leak node : \"+str(leak_node))\n",
    "\n",
    "\tif leak_node in [1, 4, 7]:\n",
    "\t\tplt.ylabel('Actual label')\n",
    "\tif leak_node in [7, 8, 9]:\n",
    "\t\tplt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "\t#create scalar file\n",
    "\n",
    "\n",
    "\tnot_scaled_df = raw_df2.copy()\n",
    "\tpop_col_not_scaled_df = ['base_demand', 'demand_value', 'head_value', 'pressure_value', 'x_pos', 'y_pos',\n",
    "\t\t\t\t\t\t\t 'flow_demand_in', 'demand_0', 'head_0', 'pressure_0',\n",
    "\t\t\t\t\t\t\t 'demand_1', 'head_1', 'pressure_1', 'demand_2', 'head_2', 'pressure_2',\n",
    "\t\t\t\t\t\t\t 'demand_3', 'head_3', 'pressure_3', 'demand_4', 'head_4', 'pressure_4',\n",
    "\t\t\t\t\t\t\t 'demand_5', 'head_5', 'pressure_5', 'demand_6', 'head_6', 'pressure_6',\n",
    "\t\t\t\t\t\t\t 'demand_7', 'head_7', 'pressure_7', 'demand_8', 'head_8', 'pressure_8',\n",
    "\t\t\t\t\t\t\t 'demand_9', 'head_9', 'pressure_9', 'flow_demand_out', 'leak_group']\n",
    "\tnot_scaled_df = not_scaled_df.drop(pop_col_not_scaled_df, axis=1)\n",
    "\n",
    "\tscaled_df = raw_df2.copy()\n",
    "\tpop_col_scaled_df = ['hour', 'nodeID', 'node_type', 'leak_area_value', 'leak_discharge_value', 'leak_demand_value', 'has_leak' ]\n",
    "\tscaled_df = scaled_df.drop(pop_col_scaled_df, axis=1)\n",
    "\t# scaled_df.rename(columns = {'leak_group':'Class'}, inplace = True)\n",
    "\tpop_col_scaled_df_2 = ['leak_group']\n",
    "\tscaled_df = scaled_df.drop(pop_col_scaled_df_2, axis=1)\n",
    "\n",
    "\tscaled_columns_names = scaled_df.columns\n",
    "\tcolumns_names_scalar = []\n",
    "\tfor kk in range(len(scaled_columns_names)):\n",
    "\t\tcolumns_names_scalar.append(scaled_columns_names[kk]+\"_scalar\")\n",
    "\n",
    "\tcleaned_df_for_scalar_new_columns = pd.DataFrame(data=scaled_df, columns=columns_names_scalar)\n",
    "\tcleaned_df_for_scalar_new_columns = scaler.transform(cleaned_df_for_scalar_new_columns)\n",
    "\n",
    "\traw_df_for_scalar_final = pd.concat([not_scaled_df, cleaned_df_for_scalar_new_columns], axis=1)\n",
    "\tout_filename = \"1M_one_res_small_leaks_ordered_group_\"+str(leak_group)+\"_node_\"+str(leak_node)+\"_\"+leak_area+\"_merged_scalar.csv\"\n",
    "\traw_df_for_scalar_final.to_csv(dataset_path+out_filename, float_format='%.8f', index=False, sep=';')\n",
    "\t!\n",
    "\tquit(1)\n",
    "\n",
    "# fig_output_filename = \"tensorflow_group_datasets/fig/\" + \"model_leak_group\" + str(leak_group) + \"_train_node_\" + str(leak_group_model) + \".png\"\n",
    "# plt.savefig(fig_output_filename, dpi=300, bbox_inches=\"tight\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-28T11:34:25.463554Z",
     "end_time": "2023-07-28T11:34:32.168640Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "dataset_path = 'tensorflow_group_datasets/one_res_small/0_no_leaks_rand_base_demand/'\n",
    "\n",
    "out_filename = '1M_one_res_small_leaks_ordered_group_0_node_0_0164_merged.csv'\n",
    "raw_df1 = pd.read_csv(dataset_path+out_filename, delimiter=\";\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-28T11:34:32.170631Z",
     "end_time": "2023-07-28T11:34:32.647622Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leak node :  7\n",
      "Examples:\n",
      "    Total: 107520\n",
      "    Positive: 6720 (6.25% of total)\n",
      "\n",
      "0\n",
      "[False]\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000018C69B1AAE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000018C69B1AAE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "[[1.1803749e-12]]\n",
      "\n",
      "1\n",
      "[ True]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "[[0.9982971]]\n",
      "\n",
      "2\n",
      "[False]\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "[[1.0182224e-06]]\n",
      "\n",
      "3\n",
      "[False]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "[[1.7410284e-07]]\n",
      "\n",
      "4\n",
      "[False]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "[[4.944453e-06]]\n",
      "\n",
      "5\n",
      "[False]\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "[[8.245003e-12]]\n",
      "\n",
      "6\n",
      "[False]\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "[[9.6663946e-08]]\n",
      "\n",
      "7\n",
      "[False]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "[[4.6795928e-11]]\n",
      "\n",
      "8\n",
      "[False]\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "[[5.207624e-11]]\n",
      "\n",
      "9\n",
      "[False]\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "[[7.604432e-17]]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1200x1000 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "leak_area = \"0164\" # \"0246\" #\"0164\"\n",
    "# 1-->7 2-->7 3-->6 4-->2 5-->1 6-->7 7-->7\n",
    "leak_group = 7\n",
    "leak_group_model = 7\n",
    "\n",
    "dataset_path = \"tensorflow_group_datasets/model/h5/\"\n",
    "model_filename = \"model_leak_group\"+str(leak_group)+\"_train_node_\"+str(leak_group_model)+\".h5\"\n",
    "loaded_model = tf.keras.models.load_model(dataset_path+model_filename)\n",
    "\n",
    "dataset_path = 'tensorflow_group_datasets/one_res_small/1_at_82_leaks_rand_base_demand/'\n",
    "\n",
    "# plt.figure(figsize=(5, 5))\n",
    "fig = plt.figure()\n",
    "\n",
    "\n",
    "for leak_node in range(7,8,1):\n",
    "\tprint(\"leak node : \", leak_node)\n",
    "\tout_filename = \"1M_one_res_small_leaks_ordered_group_\"+str(leak_group)+\"_node_\"+str(leak_node)+\"_\"+leak_area+\"_merged.csv\"\n",
    "\traw_df2 = pd.read_csv(dataset_path+out_filename, delimiter=\";\")\n",
    "\n",
    "    # Appending multiple DataFrame\n",
    "\traw_df = pd.concat([raw_df1, raw_df2])\n",
    "\traw_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\tcleaned_df = raw_df.copy()\n",
    "\tpop_col = ['hour', 'nodeID', 'node_type', 'leak_area_value', 'leak_discharge_value', 'leak_demand_value', 'has_leak' ]\n",
    "\n",
    "\tcleaned_df = cleaned_df.drop(pop_col, axis=1)\n",
    "\tcleaned_df.rename(columns = {'leak_group':'Class'}, inplace = True)\n",
    "\n",
    "\t#%%\n",
    "\tneg, pos = np.bincount(cleaned_df['Class'])\n",
    "\ttotal = neg + pos\n",
    "\tprint('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n",
    "\t\ttotal, pos, 100 * pos / total))\n",
    "\n",
    "\t#%%\n",
    "\t# Use a utility from sklearn to split and shuffle your dataset.\n",
    "\ttrain_df, test_df = train_test_split(cleaned_df, test_size=0.2)\n",
    "\ttrain_df, val_df = train_test_split(train_df, test_size=0.2)\n",
    "\n",
    "\t# Form np arrays of labels and features.\n",
    "\ttrain_labels = np.array(train_df.pop('Class'))\n",
    "\tbool_train_labels = train_labels != 0\n",
    "\tval_labels = np.array(val_df.pop('Class'))\n",
    "\ttest_labels = np.array(test_df.pop('Class'))\n",
    "\n",
    "\ttrain_features = np.array(train_df)\n",
    "\tval_features = np.array(val_df)\n",
    "\ttest_features = np.array(test_df)\n",
    "\n",
    "\t#%%\n",
    "\tscaler = StandardScaler()\n",
    "\ttrain_features = scaler.fit_transform(train_features)\n",
    "\tval_features = scaler.transform(val_features)\n",
    "\ttest_features = scaler.transform(test_features)\n",
    "\n",
    "\t#%%\n",
    "\t# pos_df = pd.DataFrame(train_features[ bool_train_labels], columns=train_df.columns)\n",
    "\t# neg_df = pd.DataFrame(train_features[~bool_train_labels], columns=train_df.columns)\n",
    "\n",
    "\n",
    "\t# baseline_results =loaded_model.evaluate(train_features, train_labels, batch_size=BATCH_SIZE, verbose=0)\n",
    "\t# for name, value in zip(loaded_model.metrics_names, baseline_results):\n",
    "\t# \tif name == \"loss\":\n",
    "\t# \t\tloss = value\n",
    "\t# \t\tprint(name, ': ', value)\n",
    "\t# \tif name == \"accuracy\":\n",
    "\t# \t\tacc = value\n",
    "\t# \tprint(name, ': ', value)\n",
    "\t# print()\n",
    "\t#\n",
    "\t#\n",
    "\t# baseline_results =loaded_model.evaluate(val_features, val_labels, batch_size=BATCH_SIZE, verbose=0)\n",
    "\t# for name, value in zip(loaded_model.metrics_names, baseline_results):\n",
    "\t# \tif name == \"loss\":\n",
    "\t# \t\tloss = value\n",
    "\t# \t\tprint(name, ': ', value)\n",
    "\t# \tif name == \"accuracy\":\n",
    "\t# \t\tacc = value\n",
    "\t# \tprint(name, ': ', value)\n",
    "\t# print()\n",
    "\n",
    "\t# baseline_results =loaded_model.evaluate(test_features, test_labels, batch_size=BATCH_SIZE, verbose=0)\n",
    "\t# for name, value in zip(loaded_model.metrics_names, baseline_results):\n",
    "\t# \t# print(name, ': ', value)\n",
    "\t# \tif name == \"loss\":\n",
    "\t# \t\tloss = value\n",
    "\t# \t\tprint(name, ': ', value)\n",
    "\t# \tif name == \"accuracy\":\n",
    "\t# \t\tacc = value\n",
    "\t# \t\tprint(name, ': ', value)\n",
    "\t#\n",
    "\t# test_predictions_baseline = loaded_model.predict(test_features, batch_size=BATCH_SIZE)\n",
    "\t#\n",
    "\t# # plot_cm(test_labels, test_predictions_baseline, \"leak node : \"+str(leak_node))\n",
    "\t# p=0.5\n",
    "\t#\n",
    "\t# cm = confusion_matrix(test_labels, test_predictions_baseline > p)\n",
    "\t# # plt.figure(figsize=(5, 5))\n",
    "\t# ax1 = fig.add_subplot(3, 3, leak_node)\n",
    "\t# sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "\t#\n",
    "\t# plt.title(\"leak node : \"+str(leak_node))\n",
    "\n",
    "\t#\n",
    "\t#\n",
    "\t# #create scalar file\n",
    "\t#\n",
    "\t#\n",
    "\t# not_scaled_df = raw_df2.copy()\n",
    "\t# pop_col_not_scaled_df = ['base_demand', 'demand_value', 'head_value', 'pressure_value', 'x_pos', 'y_pos',\n",
    "\t# \t\t\t\t\t\t 'flow_demand_in', 'demand_0', 'head_0', 'pressure_0',\n",
    "\t# \t\t\t\t\t\t 'demand_1', 'head_1', 'pressure_1', 'demand_2', 'head_2', 'pressure_2',\n",
    "\t# \t\t\t\t\t\t 'demand_3', 'head_3', 'pressure_3', 'demand_4', 'head_4', 'pressure_4',\n",
    "\t# \t\t\t\t\t\t 'demand_5', 'head_5', 'pressure_5', 'demand_6', 'head_6', 'pressure_6',\n",
    "\t# \t\t\t\t\t\t 'demand_7', 'head_7', 'pressure_7', 'demand_8', 'head_8', 'pressure_8',\n",
    "\t# \t\t\t\t\t\t 'demand_9', 'head_9', 'pressure_9', 'flow_demand_out', 'leak_group']\n",
    "\t# not_scaled_df = not_scaled_df.drop(pop_col_not_scaled_df, axis=1)\n",
    "\t#\n",
    "\t# scaled_df = raw_df2.copy()\n",
    "\t# pop_col_scaled_df = ['hour', 'nodeID', 'node_type', 'leak_area_value', 'leak_discharge_value', 'leak_demand_value', 'has_leak' ]\n",
    "\t# scaled_df = scaled_df.drop(pop_col_scaled_df, axis=1)\n",
    "\t# # scaled_df.rename(columns = {'leak_group':'Class'}, inplace = True)\n",
    "\t# pop_col_scaled_df_2 = ['leak_group']\n",
    "\t# scaled_df = scaled_df.drop(pop_col_scaled_df_2, axis=1)\n",
    "\t#\n",
    "\t# scaled_columns_names = scaled_df.columns\n",
    "\t# columns_names_scalar = []\n",
    "\t# for kk in range(len(scaled_columns_names)):\n",
    "\t# \tcolumns_names_scalar.append(scaled_columns_names[kk]+\"_scalar\")\n",
    "\t#\n",
    "\t# cleaned_df_for_scalar_new_columns = pd.DataFrame(data=scaled_df, columns=columns_names_scalar)\n",
    "\t# cleaned_df_for_scalar_new_columns = scaler.transform(cleaned_df_for_scalar_new_columns)\n",
    "\n",
    "\t# print(test2_features_2[0:1])\n",
    "\t# print(test2_labels_2[0:1])\n",
    "\tfor ii in range(10):\n",
    "\t\tprint(ii)\n",
    "\t\tprint(test_labels[ii:ii+1])\n",
    "\t\tprint(loaded_model.predict(test_features[ii:ii+1], batch_size=BATCH_SIZE))\n",
    "\t\tprint(\"\")\n",
    "\t\ttime.sleep(1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-28T11:35:41.805684Z",
     "end_time": "2023-07-28T11:35:53.415807Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x0000018C64D14950> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x0000018C64D14950> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000018C64CEC620> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000018C64CEC620> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Domenico\\AppData\\Local\\Temp\\tmp00qzpp95\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Domenico\\AppData\\Local\\Temp\\tmp00qzpp95\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leak node :  7\n",
      "Index(['leak_group', 'base_demand_scalar', 'demand_value_scalar',\n",
      "       'head_value_scalar', 'pressure_value_scalar', 'x_pos_scalar',\n",
      "       'y_pos_scalar', 'flow_demand_in_scalar', 'demand_0_scalar',\n",
      "       'head_0_scalar', 'pressure_0_scalar', 'demand_1_scalar',\n",
      "       'head_1_scalar', 'pressure_1_scalar', 'demand_2_scalar',\n",
      "       'head_2_scalar', 'pressure_2_scalar', 'demand_3_scalar',\n",
      "       'head_3_scalar', 'pressure_3_scalar', 'demand_4_scalar',\n",
      "       'head_4_scalar', 'pressure_4_scalar', 'demand_5_scalar',\n",
      "       'head_5_scalar', 'pressure_5_scalar', 'demand_6_scalar',\n",
      "       'head_6_scalar', 'pressure_6_scalar', 'demand_7_scalar',\n",
      "       'head_7_scalar', 'pressure_7_scalar', 'demand_8_scalar',\n",
      "       'head_8_scalar', 'pressure_8_scalar', 'demand_9_scalar',\n",
      "       'head_9_scalar', 'pressure_9_scalar', 'flow_demand_out_scalar'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001B[1;31mSystemExit\u001B[0m\u001B[1;31m:\u001B[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1200x1000 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "leak_area = \"0164\" # \"0246\" #\"0164\"\n",
    "# 1-->7 2-->7 3-->6 4-->2 5-->1 6-->7 7-->7\n",
    "leak_group = 7\n",
    "leak_group_model = 7\n",
    "\n",
    "dataset_path = \"tensorflow_group_datasets/model/h5/\"\n",
    "model_filename = \"model_leak_group\"+str(leak_group)+\"_train_node_\"+str(leak_group_model)+\".h5\"\n",
    "loaded_model = tf.keras.models.load_model(dataset_path+model_filename)\n",
    "\n",
    "# Convert the model.\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(loaded_model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "dataset_tflite_path = \"tensorflow_group_datasets/model/tflite/\"\n",
    "model_tflite_filename = \"model_leak_group\"+str(leak_group)+\"_train_node_\"+str(leak_group_model)+\".tflite\"\n",
    "with open(dataset_tflite_path+model_tflite_filename, 'wb') as f:\n",
    "  f.write(tflite_model)\n",
    "\n",
    "\n",
    "# dataset_path = 'tensorflow_group_datasets/one_res_small/1_at_82_leaks_rand_base_demand/'\n",
    "dataset_path = 'tensorflow_group_datasets/one_res_small/1_at_82_leaks_rand_base_demand_scalar/'\n",
    "\n",
    "# plt.figure(figsize=(5, 5))\n",
    "fig = plt.figure()\n",
    "\n",
    "\n",
    "for leak_node in range(7,8,1):\n",
    "\tprint(\"leak node : \", leak_node)\n",
    "\t# out_filename = \"1M_one_res_small_leaks_ordered_group_\"+str(leak_group)+\"_node_\"+str(leak_node)+\"_\"+leak_area+\"_merged.csv\"\n",
    "\tout_filename = \"1M_one_res_small_leaks_ordered_group_\"+str(leak_group)+\"_node_\"+str(leak_node)+\"_\"+leak_area+\"_merged_scalar.csv\"\n",
    "\traw_df2 = pd.read_csv(dataset_path+out_filename, delimiter=\";\")\n",
    "\n",
    "    # Appending multiple DataFrame\n",
    "\t# raw_df = pd.concat([raw_df1, raw_df2])\n",
    "\traw_df = pd.concat([raw_df2])\n",
    "\traw_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\tcleaned_df = raw_df.copy()\n",
    "\tpop_col = ['hour', 'nodeID', 'node_type', 'leak_area_value', 'leak_discharge_value', 'leak_demand_value', 'has_leak' ]\n",
    "\n",
    "\tcleaned_df = cleaned_df.drop(pop_col, axis=1)\n",
    "\t# cleaned_df.rename(columns = {'leak_group':'Class'}, inplace = True)\n",
    "\n",
    "\t# #%%\n",
    "\t# neg, pos = np.bincount(cleaned_df['Class'])\n",
    "\t# total = neg + pos\n",
    "\t# print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n",
    "\t# \ttotal, pos, 100 * pos / total))\n",
    "\n",
    "\t#%%\n",
    "\t# Use a utility from sklearn to split and shuffle your dataset.\n",
    "\t# train_df, test_df = train_test_split(cleaned_df, test_size=0.2)\n",
    "\t# train_df, val_df = train_test_split(train_df, test_size=0.2)\n",
    "\n",
    "\ttrain_val_test_df = cleaned_df\n",
    "\tprint(cleaned_df.columns)\n",
    "\tsys.exit(1)\n",
    "\n",
    "\t# Form np arrays of labels and features.\n",
    "\t# train_labels = np.array(train_val_test_df.pop('Class'))\n",
    "\n",
    "\ttrain_features = np.array(train_val_test_df)\n",
    "\n",
    "\t#%%\n",
    "\t# scaler = StandardScaler()\n",
    "\t# train_features = scaler.fit_transform(train_features)\n",
    "\n",
    "\t# test_predictions_baseline = loaded_model.predict(test_features, batch_size=BATCH_SIZE)\n",
    "\tp=0.3\n",
    "\t# cm = confusion_matrix(test_labels, test_predictions_baseline > p)\n",
    "\t# print(cm)\n",
    "\n",
    "\tfor ii in range(180):\n",
    "\t\tprint('******', ii, '************')\n",
    "\t\tcurrent_label = raw_df.loc[ii, [\"has_leak\"]].values\n",
    "\t\tprint(raw_df.loc[ii, [\"hour\", \"nodeID\", \"has_leak\"]].values)\n",
    "\t\t# print(raw_df.loc[ii, [\"hour\", \"nodeID\", \"has_leak\", \"leak_group\"]].values)\n",
    "\t\t# current_label = train_labels[ii]\n",
    "\t\tprint(current_label)\n",
    "\t\tcurrent_prediction = loaded_model.predict(train_features[ii:ii+1], batch_size=BATCH_SIZE)>p\n",
    "\t\tprint(current_prediction)\n",
    "\t\t# if not current_prediction == current_label:\n",
    "\t\t# \tsys.exit(1)\n",
    "\t\t# time.sleep(0.3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-22T14:06:53.394057Z",
     "end_time": "2023-06-22T14:07:11.482737Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf lite float\n",
      "[{'name': 'serving_default_dense_40_input:0', 'index': 0, 'shape': array([ 1, 38]), 'shape_signature': array([-1, 38]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "[{'name': 'StatefulPartitionedCall:0', 'index': 7, 'shape': array([1, 1]), 'shape_signature': array([-1,  1]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "****** 0 ************\n",
      "['0:00:00' '8614' False]\n",
      "[False]\n",
      "Predicted: [[2.767681e-10]]\n",
      "****** 1 ************\n",
      "['0:00:00' '8600' False]\n",
      "[False]\n",
      "Predicted: [[1.2443002e-10]]\n",
      "****** 2 ************\n",
      "['0:00:00' '8610' False]\n",
      "[False]\n",
      "Predicted: [[2.26559e-06]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_19740\\355076100.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     33\u001B[0m         \u001B[1;31m# if not current_prediction == current_label:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     34\u001B[0m         \u001B[1;31m#       sys.exit(1)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 35\u001B[1;33m         \u001B[0mtime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msleep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     36\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "#verify Tensorflow lite\n",
    "interpreter = tf.lite.Interpreter(model_path=dataset_tflite_path+model_tflite_filename)\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(\"tf lite float\")\n",
    "\n",
    "print(input_details)\n",
    "print(output_details)\n",
    "\n",
    "# for i in range(len(validation_data_set)):\n",
    "#     interpreter.set_tensor(input_details[0]['index'], validation_data_set[i])\n",
    "#     interpreter.invoke()\n",
    "#     preds_tf_lite = interpreter.get_tensor(output_details[0]['index'])\n",
    "#     print('Predicted:', decode_predictions(preds_tf_lite, top=3)[0])\n",
    "#\n",
    "#\n",
    "input_data = np.float32(train_features)\n",
    "for ii in range(180):\n",
    "\tprint('******', ii, '************')\n",
    "\tcurrent_label = raw_df.loc[ii, [\"has_leak\"]].values\n",
    "\tprint(raw_df.loc[ii, [\"hour\", \"nodeID\", \"has_leak\"]].values)\n",
    "\t# print(raw_df.loc[ii, [\"hour\", \"nodeID\", \"has_leak\", \"leak_group\"]].values)\n",
    "\t# current_label = train_labels[ii]\n",
    "\tprint(current_label)\n",
    "\t# current_prediction = loaded_model.predict(train_features[ii:ii+1], batch_size=BATCH_SIZE)>p\n",
    "\tinterpreter.set_tensor(input_details[0]['index'], input_data[ii:ii+1])\n",
    "\tinterpreter.invoke()\n",
    "\tpreds_tf_lite = interpreter.get_tensor(output_details[0]['index'])\n",
    "\tprint('Predicted:', preds_tf_lite)\n",
    "\t# print(current_prediction)\n",
    "\t# if not current_prediction == current_label:\n",
    "\t# \tsys.exit(1)\n",
    "\ttime.sleep(3)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-22T14:09:00.698261Z",
     "end_time": "2023-06-22T14:09:00.745258Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
